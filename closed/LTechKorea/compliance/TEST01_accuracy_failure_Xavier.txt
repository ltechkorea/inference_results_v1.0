In our AGX Xavier and Xavier NX non-SingleStream tests, the SUT utilizes both 
the GPU and DLA to perform inferences. These are two different accelerators
with two different numerical properties and as such, an inference performed on
one accelerator can result in a different answer from that performed on the other.
Reported test failure is due to a sample being run on one accelerator in accuracy mode 
and on the other in performance mode. To provide assurances that both 
accelerators return satisfactory accuracy, we use 
https://github.com/mlperf/inference/blob/master/compliance/nvidia/TEST01/create_accuracy_baseline.sh
to isolate the samples in the accuracy mode log that correspond to the samples 
found in the performance mode log. The accuracy metric calculation script can 
then be run on this subset of accuracy mode results in order to make a 
meaningful apples-to-apples comparison with the performance mode results. 
For example:
closed/NVIDIA/compliance/AGX_Xavier_TRT/resnet50/Offline/TEST01/accuracy/baseline_accuracy.txt
closed/NVIDIA/compliance/AGX_Xavier_TRT/resnet50/Offline/TEST01/accuracy/compliance_accuracy.txt

As shown in our logs, for every run, the two logs differ by no more than 0.4%.
